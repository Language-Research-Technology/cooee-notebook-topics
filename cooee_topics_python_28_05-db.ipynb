{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ffb99-3330-48d1-a52c-5c6ed5fe45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rocrate-tabular\n",
    "\n",
    "!pip install git+https://github.com/Sydney-Informatics-Hub/rocrate-tabular.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b5714-17a7-483b-9778-11bec6206b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os                                                  # Functions for interacting with the operating system.\n",
    "import zipfile                                             # Tools to create, read, write, append and list a ZIP file.\n",
    "import requests                                            # Send HTTP requests.\n",
    "from io import BytesIO                                     # Perform file operations on byte data.\n",
    "from rocrate_tabular.tabulator import ROCrateTabulator     # Python library to turn an RO-Crate into tabular formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb88ca-72d6-4595-a8b6-8c317f4b4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the names of the database, folder and configuration file to be created, or leave as the defaults.\n",
    "\n",
    "database = 'cooee.db'     # Edit the section in quotes to rename the database.\n",
    "folder = 'cooee'          # Edit the section in quotes to rename the folder that is created for the database.\n",
    "config = 'config.json'    # Edit the section in quotes to rename the configuration file to generate the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbc97e-3bd0-46d3-b830-2ee7654d7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the COOEE collection ZIP from the LDaCA data portal and extract it to a folder in the current working directory.\n",
    "\n",
    "zip_url = \"https://data.ldaca.edu.au/api/object/arcp%3A%2F%2Fname%2Chdl10.26180~23961609.zip\"\n",
    "cwd = os.getcwd()\n",
    "extract_to = os.path.join(cwd, folder)\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "response = requests.get(zip_url, stream=True)\n",
    "response.raise_for_status()\n",
    "with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "    zip_ref.extractall(extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87b75b-4c90-4fb3-bd4c-a6a126d36de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the RO-Crate to a database. Arguments specified are the RO-Crate directory and the output name of the database.\n",
    "# output saved to object because (at least for WIN32) database has to be explicitly closed before update below\n",
    "\n",
    "tb = ROCrateTabulator()\n",
    "data_base = tb.crate_to_db(folder, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6d2310-bd1f-4eac-ada5-dc1164df4f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create or update the config file, specifying that `indexableText` is the entity containing text data for COOEE.\n",
    "\n",
    "if os.path.exists(config):\n",
    "    tb.load_config(config)\n",
    "    print(\"load\")\n",
    "else:\n",
    "    tb.infer_config()\n",
    "    print(\"infer\")\n",
    "\n",
    "for table in tb.cf[\"tables\"]:\n",
    "    print(f\"Building entity table for {table}\")\n",
    "    tb.entity_table(table, 'indexableText')\n",
    "\n",
    "tb.write_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b0580-76f1-4d4f-bd93-81b604085822",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Once the `config.json` file is generated, right-click it in the File Browser and select 'Open With' > 'Editor'.\n",
    "\n",
    "Replace the section `\"tables\": {},` with the following:\n",
    "```\n",
    "    \"tables\": {\n",
    "        \"RepositoryObject\": {\n",
    "            \"all_props\": [],\n",
    "            \"ignore_props\": [],\n",
    "            \"expand_props\": []\n",
    "        }\n",
    "    },\n",
    "```\n",
    "<br>\n",
    "\n",
    "This indicates that we want to use the `RepositoryObject` class to generate the table in the database.\n",
    "\n",
    "Then remove the following section from `potential_tables`:\n",
    "```\n",
    "        \"RepositoryObject\": {\n",
    "            \"all_props\": [],\n",
    "            \"ignore_props\": [],\n",
    "            \"expand_props\": []\n",
    "        },\n",
    "```\n",
    "<br>\n",
    "\n",
    "Save `config.json` and close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b33a2-9589-47fb-bef7-54c13d49fc00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a252cf-0902-49fc-93da-cedce58d01a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now that the config table has been updated, re-generate the database output.\n",
    "\n",
    "# explicit clsoing of database for WIN32\n",
    "data_base.close()\n",
    "\n",
    "# regenerate - does this output need to be assigned to an object also? There is another possible regeneration step...\n",
    "tb.crate_to_db(folder, database)\n",
    "\n",
    "if os.path.exists(config):\n",
    "    tb.load_config(config)\n",
    "    print(\"load\")\n",
    "else:\n",
    "    tb.infer_config()\n",
    "    print(\"infer\")\n",
    "\n",
    "for table in tb.cf[\"tables\"]:\n",
    "    print(f\"Building entity table for {table}\")\n",
    "    tb.entity_table(table, 'indexableText')\n",
    "\n",
    "tb.write_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc6721-9993-4194-98e3-fc1b3ea04073",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Open `config.json` again. The `all_props` section should be populated. If you need to do a subquery on a target ID to make expanded properties such as `author_name` and `author_id`, copy the required properties to the `expand_props` section. For example:\n",
    "\n",
    "```\n",
    "            \"expand_props\": [\n",
    "                \"author\",\n",
    "                \"register\",\n",
    "                \"recipient\"\n",
    "            ]\n",
    "```\n",
    "<br>\n",
    "\n",
    "This indicates that we want the `author`, `register` and `recipient` properties to be expanded in the database.\n",
    "\n",
    "Save `config.json` and close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf7230-279f-4dce-b21b-f34302c7c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-generate the database output to include the expanded properties in the database.\n",
    "\n",
    "\n",
    "tb.crate_to_db(folder, database)\n",
    "\n",
    "if os.path.exists(config):\n",
    "    tb.load_config(config)\n",
    "    print(\"load\")\n",
    "else:\n",
    "    tb.infer_config()\n",
    "    print(\"infer\")\n",
    "\n",
    "for table in tb.cf[\"tables\"]:\n",
    "    print(f\"Building entity table for {table}\")\n",
    "    tb.entity_table(table, 'indexableText')\n",
    "\n",
    "tb.write_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0e067-5432-4756-a7ad-64372e8d6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cooee.db version\n",
    "\n",
    "import sqlite3\n",
    "import pandas\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(database)\n",
    "\n",
    "# Write an SQL query to select data\n",
    "query = \"SELECT * FROM RepositoryObject\"  # Replace with your table name if not using RepositoryObject\n",
    "\n",
    "# Read data into a Pandas DataFrame\n",
    "df = pandas.read_sql_query(query, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "#Remove the first row of the DataFrame\n",
    "cooee = df.iloc[1:]\n",
    "\n",
    "# Remove rows where 'indexableText' column has NaN values\n",
    "#cooee = df.dropna(subset=['indexableText'])\n",
    "\n",
    "# Display the DataFrame info\n",
    "cooee.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d4e0d-6ef0-465a-aa10-4795dd95732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame where 'indexableText' is NaN\n",
    "filtered_df = cooee[cooee['indexableText'].isna()]\n",
    "\n",
    "# Select only 'entity_id' and 'indexableText' columns\n",
    "result = filtered_df[['entity_id', 'name', 'indexableText']]\n",
    "\n",
    "# Display the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7414c-d1fd-40ef-bfa8-423ff2be0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratification data to split cooee data frame into 16 subsets\n",
    "\n",
    "registers = {\"full\": [\"Government English\", \"Private Written\", \"Public Written\", \"Speech Based\"], \"short\": [\"ge\", \"prw\", \"puw\", \"sb\"]}\n",
    "registers = pandas.DataFrame(registers)\n",
    "print(registers)\n",
    "\n",
    "periods = {\"period\": [1,2,3,4], \"start\": [1788, 1826, 1851, 1876], \"end\": [1825, 1850, 1875, 1900]}\n",
    "time_periods = pandas.DataFrame(periods)\n",
    "print(time_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07b20f-f49c-49d4-ad11-cfcb54d4b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cooee.db version\n",
    "\n",
    "## make list of titles for slices and list with combined texts for each slice\n",
    "sub_titles = []\n",
    "documents = []\n",
    "\n",
    "for i in range(0, 4):\n",
    "    for j in range(0, 4):\n",
    "        sub_title = registers.iloc[i, 1] + \"_period\" + str(time_periods.iloc[j, 0])\n",
    "        sub_titles.append(sub_title)\n",
    "        \n",
    "        # Ensure the values for time_periods are numeric\n",
    "        start_period = pandas.to_numeric(time_periods.iloc[j, 1], errors='coerce')  # Convert to numeric\n",
    "        end_period = pandas.to_numeric(time_periods.iloc[j, 2], errors='coerce')    # Convert to numeric\n",
    "        \n",
    "        # Convert column 19 of cooee DataFrame to numeric values (errors='coerce' will turn non-numeric into NaN)\n",
    "        cooee.iloc[:, 19] = pandas.to_numeric(cooee.iloc[:, 19], errors='coerce')\n",
    "        \n",
    "        # Filter cooee DataFrame based on conditions\n",
    "        temp = cooee.loc[\n",
    "            (cooee[\"register\"] == registers.iloc[i, 0]) & \n",
    "            (cooee.iloc[:, 19] >= start_period) &  # Ensure comparison with numeric values\n",
    "            (cooee.iloc[:, 19] <= end_period)    # Ensure comparison with numeric values\n",
    "        ]\n",
    "        # export indexableText to list\n",
    "        index_text = temp['indexableText'].values.tolist()\n",
    "        # combine indexableText as single document\n",
    "        texts = \"\"\n",
    "        for text in index_text:\n",
    "            texts = texts + str(text)\n",
    "        \n",
    "        documents.append(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafefcbe-9a63-4864-8a09-a46cece23739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning, tokenization, lemmatization\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "\n",
    "\n",
    "punctuation = u\",.?!()-_\\\"\\'\\\\\\n\\r\\t;:+*<>@#ยง^$%&|/\"\n",
    "stop_words_eng = set(stopwords.words('english'))\n",
    "# add to stop list: apostrophe-s, non-standard quotation marks, 'would'\n",
    "stop_words_eng.update([\"'s\", \"``\", \"would\", \"''\" ])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tag_dict = {\"J\": wn.ADJ,\n",
    "            \"N\": wn.NOUN,\n",
    "            \"V\": wn.VERB,\n",
    "            \"R\": wn.ADV}\n",
    "\n",
    "def extract_wnpostag_from_postag(tag):\n",
    "    #take the first letter of the tag\n",
    "    #the second parameter is an \"optional\" in case of missing key in the dictionary \n",
    "    return tag_dict.get(tag[0].upper(), None)\n",
    "\n",
    "def lemmatize_tupla_word_postag(tupla):\n",
    "    \"\"\"\n",
    "    giving a tupla of the form (wordString, posTagString) like ('guitar', 'NN'), return the lemmatized word\n",
    "    \"\"\"\n",
    "    tag = extract_wnpostag_from_postag(tupla[1])    \n",
    "    return lemmatizer.lemmatize(tupla[0], tag) if tag is not None else tupla[0]\n",
    "\n",
    "def bag_of_words(sentence, stop_words=None):\n",
    "    if stop_words is None:\n",
    "        stop_words = stop_words_eng\n",
    "    original_words = word_tokenize(sentence)\n",
    "    lower_case = [word.lower() for word in original_words]\n",
    "    tagged_words = nltk.pos_tag(lower_case) #returns a list of tuples: (word, tagString) like ('And', 'CC')\n",
    "    original_words = None\n",
    "    lemmatized_words = [ lemmatize_tupla_word_postag(ow) for ow in tagged_words ]\n",
    "    tagged_words = None\n",
    "    cleaned_words = [ w for w in lemmatized_words if (w not in punctuation) and (w not in stop_words) ]\n",
    "    lemmatized_words = None\n",
    "    no_numbers = [re.sub(r'\\w*\\d\\w*', '', w) for w in cleaned_words]\n",
    "    return no_numbers\n",
    "\n",
    "clean_documents = []\n",
    "\n",
    "for i in range(0,len(documents)):\n",
    "    # hack to get rid of new line characters\n",
    "    text_1 = documents[i].replace('\\\\', 'q')\n",
    "    text_2 = text_1.replace('qn', ' ')\n",
    "    text_3 = text_2.replace(\"'s\", '')\n",
    "    cleaned_tokens = bag_of_words(text_2)\n",
    "    clean_documents.append(cleaned_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6fec8-1a95-477d-9f15-c6d5a402a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018f261-e26b-43c0-a35f-61c025fa0b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary and corpus\n",
    "dictionary = Dictionary(clean_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in clean_documents]\n",
    "\n",
    "# Running LDA TO DO: check parameters especailly passes, 10 topics seems about right for this data\n",
    "lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=100, random_state=100)\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017e1df-31f1-4cc0-a312-5eddc75b6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tabular display of top 10 words for each topic\n",
    "\n",
    "top_words_per_topic = []\n",
    "for t in range(lda_model.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 10)])\n",
    "\n",
    "# top_words_per_topic\n",
    "top_words = pandas.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P'])\n",
    "\n",
    "# tabular display\n",
    "topics_table = pandas.DataFrame()\n",
    "for i in range(0,10):\n",
    "    col_name = \"Topic\" + str(i+1)\n",
    "    temp = top_words.loc[(top_words[\"Topic\"] == i)]\n",
    "    temp_words = temp[\"Word\"].to_list()\n",
    "    topics_table[col_name] = temp_words\n",
    "topics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688e0cf-b8c7-4a6b-a7f0-0c7c473e3114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the 10 words per topic\n",
    "\n",
    "topics_table.to_csv(\"C:/Users/Simon/Desktop/cooee/run_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860cd68-9fc3-46c2-a93f-203805c73dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get weightings for each document\n",
    "\n",
    "doc_weights = []\n",
    "\n",
    "for doc in clean_documents:\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    t = lda_model.get_document_topics(bow, minimum_probability=0)\n",
    "    doc_weights.append(t)\n",
    "\n",
    "# drop document numbers from weights list\n",
    "weights = []\n",
    "\n",
    "for doc_row in doc_weights:\n",
    "\n",
    "    out = []\n",
    "\n",
    "    for item in doc_row:\n",
    "        weight = item[1]\n",
    "        out.append(weight)\n",
    "    weights.append(out)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a2eb1-a28f-45ff-8dc1-a477bab19bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    topic_name = \"Topic\"+ str(i+1) + \" \" + topics_table.iloc[1, i] + \" \" + topics_table.iloc[2, i]  +\" \" + topics_table.iloc[3, i]\n",
    "    topic_names.append(topic_name)\n",
    "\n",
    "topic_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60aa98-54ba-4bed-b520-431f2489aed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# massage data to be input for visualisation\n",
    "\n",
    "topic_df = (pandas.DataFrame(weights, columns= topic_names))\n",
    "\n",
    "topics_transpose = topic_df.transpose()\n",
    "topics_transpose.columns = sub_titles       \n",
    "# Output the DataFrame\n",
    "print(topics_transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4297fb-0d58-43e9-8812-112326dbe06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98097d53-54be-4320-82b9-7ed3a7eddda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 6))  # Adjust the width and height\n",
    "sns.heatmap(topics_transpose, \n",
    "            cmap='Reds',  # 'Reds' colormap corresponds to the red color scheme\n",
    "            cbar_kws={'label': 'Topic Weight'},  # Color bar label\n",
    "            linewidths=0,  # No lines between cells\n",
    "            xticklabels=True,  # Show column labels\n",
    "            yticklabels=True,  # Show row labels\n",
    "            square=False,  # To avoid forcing the aspect ratio to be square\n",
    "            cbar=True)  # Display color bar\n",
    "\n",
    "# Rotate column labels\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Save the heatmap to a PDF file\n",
    "# plt.savefig(\"results/convo_topic_heatmap.pdf\", format=\"pdf\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb633f-da0a-42f3-9a1e-e28c2acd0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output of cleaning for manual checking\n",
    "\n",
    "filename = 'clean_documents_sm_2.txt'\n",
    "outfile = open(filename, 'w')\n",
    "for list in clean_documents:\n",
    "    outfile.writelines([str(i)+'\\n' for i in list])\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
